{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edmond292000/Recognition/blob/main/DawnBench.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "# hyper parameters\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001\n",
        "channels = 32\n",
        "depth = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # shortcut\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Cifar10(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Cifar10, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.layer1 = self._make_layer(32, 32, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 64, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer5 = self._make_layer(256, 512, 2, stride=2)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, 1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        #x = F.dropout2d(x, 0.2, training=self.training)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        #x = F.dropout2d(x, 0.25, training=self.training)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        #x = F.dropout2d(x, 0.3, training=self.training)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        #x = F.dropout2d(x, 0.3, training=self.training)\n",
        "\n",
        "        x = self.layer5(x)\n",
        "        # x = F.dropout2d(x, 0.25, training=self.training)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        #x = F.dropout(x, 0.6, training=self.training)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "model = Cifar10(num_classes)\n",
        "model = model.to(device)\n",
        "print(next(model.parameters()).device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007, weight_decay=1e-4)\n",
        "# cheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "losses = []\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for img, label in trainloader:\n",
        "        img, label = img.to(device), label.to(device)\n",
        "\n",
        "        output = model(img)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += label.size(0)\n",
        "        correct += predicted.eq(label).sum().item()\n",
        "\n",
        "    train_accuracy = 100.0 * correct / total\n",
        "    avg_loss = running_loss / len(trainloader)\n",
        "    losses.append(avg_loss)\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "\n",
        "    epoch_end_time = time.time() - epoch_start_time\n",
        "    print(\n",
        "        f\"Epoch [{epoch + 1}/{num_epochs}], \"f\"Loss: {running_loss / len(trainloader):.4f}, \"f\"Train Accuracy: {train_accuracy:.2f}%\",\n",
        "        f\"Time: {epoch_end_time}: \")\n",
        "\n",
        "    # ---Testing---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        for img, label in testloader:\n",
        "            img, label = img.to(device), label.to(device)\n",
        "            output = model(img)\n",
        "            _, predicted = output.max(1)\n",
        "            total_test += label.size(0)\n",
        "            correct_test += predicted.eq(label).sum().item()\n",
        "\n",
        "        test_accuracy = 100.0 * correct_test / total_test\n",
        "        test_accuracy_list.append(test_accuracy)\n",
        "\n",
        "        print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "        if test_accuracy >= 93.0:\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(f\"Achieving 94% in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "total_training_time = time.time() - start_time\n",
        "print(f\"Total {total_training_time / 60} minutes\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(losses, label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_accuracy_list, label=\"Train Accuracy\")\n",
        "plt.plot(test_accuracy_list, label=\"Test Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Train vs Test Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AQ-pbbQ8liAk",
        "outputId": "013d75ea-c312-4b4f-9b49-b96899058eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Epoch [1/200], Loss: 1.4686, Train Accuracy: 45.83% Time: 8.601047277450562: \n",
            "Test Accuracy: 56.03%\n",
            "Epoch [2/200], Loss: 1.0049, Train Accuracy: 64.05% Time: 6.3609161376953125: \n",
            "Test Accuracy: 65.76%\n",
            "Epoch [3/200], Loss: 0.7874, Train Accuracy: 72.22% Time: 6.722705364227295: \n",
            "Test Accuracy: 76.25%\n",
            "Epoch [4/200], Loss: 0.6721, Train Accuracy: 76.39% Time: 6.9357452392578125: \n",
            "Test Accuracy: 74.97%\n",
            "Epoch [5/200], Loss: 0.5920, Train Accuracy: 79.50% Time: 6.736664056777954: \n",
            "Test Accuracy: 78.22%\n",
            "Epoch [6/200], Loss: 0.5392, Train Accuracy: 81.41% Time: 6.504906415939331: \n",
            "Test Accuracy: 80.12%\n",
            "Epoch [7/200], Loss: 0.4938, Train Accuracy: 82.88% Time: 6.596717834472656: \n",
            "Test Accuracy: 78.89%\n",
            "Epoch [8/200], Loss: 0.4582, Train Accuracy: 84.26% Time: 6.504001140594482: \n",
            "Test Accuracy: 82.77%\n",
            "Epoch [9/200], Loss: 0.4352, Train Accuracy: 85.11% Time: 6.572409629821777: \n",
            "Test Accuracy: 83.34%\n",
            "Epoch [10/200], Loss: 0.4046, Train Accuracy: 86.00% Time: 6.4492528438568115: \n",
            "Test Accuracy: 84.54%\n",
            "Epoch [11/200], Loss: 0.3881, Train Accuracy: 86.63% Time: 6.637191295623779: \n",
            "Test Accuracy: 84.24%\n",
            "Epoch [12/200], Loss: 0.3687, Train Accuracy: 87.26% Time: 6.6001036167144775: \n",
            "Test Accuracy: 81.85%\n",
            "Epoch [13/200], Loss: 0.3513, Train Accuracy: 87.96% Time: 6.620498418807983: \n",
            "Test Accuracy: 85.64%\n",
            "Epoch [14/200], Loss: 0.3299, Train Accuracy: 88.66% Time: 6.3368072509765625: \n",
            "Test Accuracy: 83.31%\n",
            "Epoch [15/200], Loss: 0.3225, Train Accuracy: 88.97% Time: 6.585397481918335: \n",
            "Test Accuracy: 86.75%\n",
            "Epoch [16/200], Loss: 0.3025, Train Accuracy: 89.66% Time: 6.591295003890991: \n",
            "Test Accuracy: 85.29%\n",
            "Epoch [17/200], Loss: 0.2906, Train Accuracy: 90.03% Time: 6.979685306549072: \n",
            "Test Accuracy: 87.19%\n",
            "Epoch [18/200], Loss: 0.2806, Train Accuracy: 90.38% Time: 6.686886548995972: \n",
            "Test Accuracy: 87.89%\n",
            "Epoch [19/200], Loss: 0.2738, Train Accuracy: 90.57% Time: 6.371873617172241: \n",
            "Test Accuracy: 86.36%\n",
            "Epoch [20/200], Loss: 0.2582, Train Accuracy: 91.08% Time: 6.578989744186401: \n",
            "Test Accuracy: 87.80%\n",
            "Epoch [21/200], Loss: 0.2509, Train Accuracy: 91.36% Time: 6.382960081100464: \n",
            "Test Accuracy: 87.12%\n",
            "Epoch [22/200], Loss: 0.2416, Train Accuracy: 91.79% Time: 6.27671217918396: \n",
            "Test Accuracy: 87.89%\n",
            "Epoch [23/200], Loss: 0.2335, Train Accuracy: 92.00% Time: 6.601931095123291: \n",
            "Test Accuracy: 87.39%\n",
            "Epoch [24/200], Loss: 0.2255, Train Accuracy: 92.17% Time: 6.878348112106323: \n",
            "Test Accuracy: 88.77%\n",
            "Epoch [25/200], Loss: 0.2165, Train Accuracy: 92.51% Time: 6.272061109542847: \n",
            "Test Accuracy: 88.79%\n",
            "Epoch [26/200], Loss: 0.2111, Train Accuracy: 92.79% Time: 6.5334227085113525: \n",
            "Test Accuracy: 88.47%\n",
            "Epoch [27/200], Loss: 0.2023, Train Accuracy: 93.02% Time: 6.219939470291138: \n",
            "Test Accuracy: 88.42%\n",
            "Epoch [28/200], Loss: 0.1989, Train Accuracy: 93.14% Time: 6.483204364776611: \n",
            "Test Accuracy: 87.34%\n",
            "Epoch [29/200], Loss: 0.1905, Train Accuracy: 93.49% Time: 6.618655681610107: \n",
            "Test Accuracy: 88.25%\n",
            "Epoch [30/200], Loss: 0.1896, Train Accuracy: 93.46% Time: 6.491079330444336: \n",
            "Test Accuracy: 89.46%\n",
            "Epoch [31/200], Loss: 0.1865, Train Accuracy: 93.60% Time: 6.45975399017334: \n",
            "Test Accuracy: 87.19%\n",
            "Epoch [32/200], Loss: 0.1764, Train Accuracy: 93.89% Time: 6.529196500778198: \n",
            "Test Accuracy: 87.57%\n",
            "Epoch [33/200], Loss: 0.1714, Train Accuracy: 94.03% Time: 6.277369737625122: \n",
            "Test Accuracy: 88.95%\n",
            "Epoch [34/200], Loss: 0.1685, Train Accuracy: 94.11% Time: 6.624882221221924: \n",
            "Test Accuracy: 89.34%\n",
            "Epoch [35/200], Loss: 0.1670, Train Accuracy: 94.28% Time: 6.430657148361206: \n",
            "Test Accuracy: 88.81%\n",
            "Epoch [36/200], Loss: 0.1614, Train Accuracy: 94.47% Time: 6.472894906997681: \n",
            "Test Accuracy: 86.73%\n",
            "Epoch [37/200], Loss: 0.1619, Train Accuracy: 94.28% Time: 6.690710544586182: \n",
            "Test Accuracy: 89.41%\n",
            "Epoch [38/200], Loss: 0.1575, Train Accuracy: 94.49% Time: 6.4222283363342285: \n",
            "Test Accuracy: 89.29%\n",
            "Epoch [39/200], Loss: 0.1498, Train Accuracy: 94.77% Time: 6.3319456577301025: \n",
            "Test Accuracy: 88.93%\n",
            "Epoch [40/200], Loss: 0.1476, Train Accuracy: 94.81% Time: 6.458611726760864: \n",
            "Test Accuracy: 89.63%\n",
            "Epoch [41/200], Loss: 0.1409, Train Accuracy: 95.16% Time: 6.290344953536987: \n",
            "Test Accuracy: 89.67%\n",
            "Epoch [42/200], Loss: 0.1423, Train Accuracy: 95.10% Time: 6.317870378494263: \n",
            "Test Accuracy: 90.02%\n",
            "Epoch [43/200], Loss: 0.1372, Train Accuracy: 95.21% Time: 6.221633434295654: \n",
            "Test Accuracy: 89.75%\n",
            "Epoch [44/200], Loss: 0.1393, Train Accuracy: 95.11% Time: 6.555614471435547: \n",
            "Test Accuracy: 89.85%\n",
            "Epoch [45/200], Loss: 0.1345, Train Accuracy: 95.35% Time: 6.684953451156616: \n",
            "Test Accuracy: 90.02%\n",
            "Epoch [46/200], Loss: 0.1338, Train Accuracy: 95.39% Time: 6.49105429649353: \n",
            "Test Accuracy: 89.63%\n",
            "Epoch [47/200], Loss: 0.1325, Train Accuracy: 95.43% Time: 6.497270822525024: \n",
            "Test Accuracy: 89.94%\n",
            "Epoch [48/200], Loss: 0.1271, Train Accuracy: 95.57% Time: 6.622179985046387: \n",
            "Test Accuracy: 89.59%\n",
            "Epoch [49/200], Loss: 0.1258, Train Accuracy: 95.61% Time: 6.4892895221710205: \n",
            "Test Accuracy: 89.69%\n",
            "Epoch [50/200], Loss: 0.1271, Train Accuracy: 95.65% Time: 6.779176712036133: \n",
            "Test Accuracy: 90.07%\n",
            "Epoch [51/200], Loss: 0.1208, Train Accuracy: 95.77% Time: 6.3862011432647705: \n",
            "Test Accuracy: 88.94%\n",
            "Epoch [52/200], Loss: 0.1179, Train Accuracy: 95.93% Time: 6.578455924987793: \n",
            "Test Accuracy: 89.89%\n",
            "Epoch [53/200], Loss: 0.1197, Train Accuracy: 95.80% Time: 6.634144067764282: \n",
            "Test Accuracy: 90.54%\n",
            "Epoch [54/200], Loss: 0.1198, Train Accuracy: 95.79% Time: 6.3544065952301025: \n",
            "Test Accuracy: 90.33%\n",
            "Epoch [55/200], Loss: 0.1178, Train Accuracy: 95.87% Time: 6.602092266082764: \n",
            "Test Accuracy: 89.86%\n",
            "Epoch [56/200], Loss: 0.1153, Train Accuracy: 96.06% Time: 6.461628675460815: \n",
            "Test Accuracy: 89.54%\n",
            "Epoch [57/200], Loss: 0.1105, Train Accuracy: 96.22% Time: 6.817058324813843: \n",
            "Test Accuracy: 90.02%\n",
            "Epoch [58/200], Loss: 0.1131, Train Accuracy: 96.13% Time: 6.432821750640869: \n",
            "Test Accuracy: 90.08%\n",
            "Epoch [59/200], Loss: 0.1120, Train Accuracy: 96.11% Time: 6.3784167766571045: \n",
            "Test Accuracy: 90.04%\n",
            "Epoch [60/200], Loss: 0.1111, Train Accuracy: 96.11% Time: 6.222476243972778: \n",
            "Test Accuracy: 89.48%\n",
            "Epoch [61/200], Loss: 0.1059, Train Accuracy: 96.33% Time: 6.526250600814819: \n",
            "Test Accuracy: 90.03%\n",
            "Epoch [62/200], Loss: 0.1089, Train Accuracy: 96.22% Time: 6.837058782577515: \n",
            "Test Accuracy: 90.14%\n",
            "Epoch [63/200], Loss: 0.1049, Train Accuracy: 96.34% Time: 6.263847351074219: \n",
            "Test Accuracy: 89.10%\n",
            "Epoch [64/200], Loss: 0.1034, Train Accuracy: 96.40% Time: 6.619576930999756: \n",
            "Test Accuracy: 89.69%\n",
            "Epoch [65/200], Loss: 0.1052, Train Accuracy: 96.24% Time: 6.609481334686279: \n",
            "Test Accuracy: 89.22%\n",
            "Epoch [66/200], Loss: 0.1001, Train Accuracy: 96.50% Time: 6.391495227813721: \n",
            "Test Accuracy: 90.33%\n",
            "Epoch [67/200], Loss: 0.1033, Train Accuracy: 96.45% Time: 6.555861711502075: \n",
            "Test Accuracy: 89.65%\n",
            "Epoch [68/200], Loss: 0.0995, Train Accuracy: 96.52% Time: 6.5338170528411865: \n",
            "Test Accuracy: 89.64%\n",
            "Epoch [69/200], Loss: 0.0997, Train Accuracy: 96.55% Time: 6.47751522064209: \n",
            "Test Accuracy: 89.87%\n",
            "Epoch [70/200], Loss: 0.0979, Train Accuracy: 96.57% Time: 6.5015764236450195: \n",
            "Test Accuracy: 90.80%\n",
            "Epoch [71/200], Loss: 0.0981, Train Accuracy: 96.44% Time: 6.093160152435303: \n",
            "Test Accuracy: 89.99%\n",
            "Epoch [72/200], Loss: 0.0964, Train Accuracy: 96.63% Time: 6.716534376144409: \n",
            "Test Accuracy: 90.44%\n",
            "Epoch [73/200], Loss: 0.1002, Train Accuracy: 96.49% Time: 6.526047468185425: \n",
            "Test Accuracy: 89.14%\n",
            "Epoch [74/200], Loss: 0.0926, Train Accuracy: 96.85% Time: 6.543685436248779: \n",
            "Test Accuracy: 89.83%\n",
            "Epoch [75/200], Loss: 0.0957, Train Accuracy: 96.61% Time: 6.423522710800171: \n",
            "Test Accuracy: 89.67%\n",
            "Epoch [76/200], Loss: 0.0934, Train Accuracy: 96.78% Time: 6.529735803604126: \n",
            "Test Accuracy: 90.15%\n",
            "Epoch [77/200], Loss: 0.0923, Train Accuracy: 96.84% Time: 6.421820163726807: \n",
            "Test Accuracy: 90.26%\n",
            "Epoch [78/200], Loss: 0.0958, Train Accuracy: 96.72% Time: 6.556666374206543: \n",
            "Test Accuracy: 90.31%\n",
            "Epoch [79/200], Loss: 0.0908, Train Accuracy: 96.90% Time: 6.756411075592041: \n",
            "Test Accuracy: 90.19%\n",
            "Epoch [80/200], Loss: 0.0943, Train Accuracy: 96.65% Time: 6.382731199264526: \n",
            "Test Accuracy: 88.98%\n",
            "Epoch [81/200], Loss: 0.0902, Train Accuracy: 96.94% Time: 6.364198446273804: \n",
            "Test Accuracy: 89.94%\n",
            "Epoch [82/200], Loss: 0.0960, Train Accuracy: 96.68% Time: 6.164331436157227: \n",
            "Test Accuracy: 90.65%\n",
            "Epoch [83/200], Loss: 0.0892, Train Accuracy: 96.95% Time: 6.6983888149261475: \n",
            "Test Accuracy: 90.49%\n",
            "Epoch [84/200], Loss: 0.0887, Train Accuracy: 96.97% Time: 6.2866222858428955: \n",
            "Test Accuracy: 89.26%\n",
            "Epoch [85/200], Loss: 0.0900, Train Accuracy: 96.85% Time: 6.7198615074157715: \n",
            "Test Accuracy: 90.49%\n",
            "Epoch [86/200], Loss: 0.0902, Train Accuracy: 96.90% Time: 6.580224990844727: \n",
            "Test Accuracy: 90.59%\n",
            "Epoch [87/200], Loss: 0.0904, Train Accuracy: 96.92% Time: 6.393804311752319: \n",
            "Test Accuracy: 90.46%\n",
            "Epoch [88/200], Loss: 0.0857, Train Accuracy: 97.06% Time: 6.528650522232056: \n",
            "Test Accuracy: 90.30%\n",
            "Epoch [89/200], Loss: 0.0870, Train Accuracy: 96.99% Time: 6.350879669189453: \n",
            "Test Accuracy: 89.88%\n",
            "Epoch [90/200], Loss: 0.0884, Train Accuracy: 97.01% Time: 6.549456834793091: \n",
            "Test Accuracy: 90.58%\n",
            "Epoch [91/200], Loss: 0.0900, Train Accuracy: 96.88% Time: 6.49461817741394: \n",
            "Test Accuracy: 90.20%\n",
            "Epoch [92/200], Loss: 0.0877, Train Accuracy: 96.96% Time: 6.189302444458008: \n",
            "Test Accuracy: 89.87%\n",
            "Epoch [93/200], Loss: 0.0847, Train Accuracy: 96.99% Time: 6.323403358459473: \n",
            "Test Accuracy: 90.60%\n",
            "Epoch [94/200], Loss: 0.0870, Train Accuracy: 97.00% Time: 6.528834581375122: \n",
            "Test Accuracy: 90.21%\n",
            "Epoch [95/200], Loss: 0.0850, Train Accuracy: 97.09% Time: 6.474127769470215: \n",
            "Test Accuracy: 90.06%\n",
            "Epoch [96/200], Loss: 0.0843, Train Accuracy: 97.05% Time: 6.629695177078247: \n",
            "Test Accuracy: 90.50%\n",
            "Epoch [97/200], Loss: 0.0820, Train Accuracy: 97.20% Time: 6.4152772426605225: \n",
            "Test Accuracy: 90.86%\n",
            "Epoch [98/200], Loss: 0.0821, Train Accuracy: 97.13% Time: 6.293406009674072: \n",
            "Test Accuracy: 90.75%\n",
            "Epoch [99/200], Loss: 0.0850, Train Accuracy: 97.05% Time: 6.946755886077881: \n",
            "Test Accuracy: 90.37%\n",
            "Epoch [100/200], Loss: 0.0861, Train Accuracy: 97.07% Time: 6.447812557220459: \n",
            "Test Accuracy: 90.11%\n",
            "Epoch [101/200], Loss: 0.0818, Train Accuracy: 97.13% Time: 6.270577669143677: \n",
            "Test Accuracy: 90.89%\n",
            "Epoch [102/200], Loss: 0.0800, Train Accuracy: 97.24% Time: 6.408973932266235: \n",
            "Test Accuracy: 89.80%\n",
            "Epoch [103/200], Loss: 0.0817, Train Accuracy: 97.19% Time: 6.4563658237457275: \n",
            "Test Accuracy: 90.15%\n",
            "Epoch [104/200], Loss: 0.0834, Train Accuracy: 97.11% Time: 6.551068067550659: \n",
            "Test Accuracy: 91.06%\n",
            "Epoch [105/200], Loss: 0.0788, Train Accuracy: 97.33% Time: 6.481042146682739: \n",
            "Test Accuracy: 90.78%\n",
            "Epoch [106/200], Loss: 0.0844, Train Accuracy: 97.10% Time: 6.373941659927368: \n",
            "Test Accuracy: 89.25%\n",
            "Epoch [107/200], Loss: 0.0836, Train Accuracy: 97.14% Time: 6.298391103744507: \n",
            "Test Accuracy: 90.88%\n",
            "Epoch [108/200], Loss: 0.0806, Train Accuracy: 97.19% Time: 6.302680730819702: \n",
            "Test Accuracy: 90.21%\n",
            "Epoch [109/200], Loss: 0.0830, Train Accuracy: 97.09% Time: 6.4006407260894775: \n",
            "Test Accuracy: 90.26%\n",
            "Epoch [110/200], Loss: 0.0788, Train Accuracy: 97.28% Time: 6.483534336090088: \n",
            "Test Accuracy: 90.26%\n",
            "Epoch [111/200], Loss: 0.0797, Train Accuracy: 97.16% Time: 6.413955211639404: \n",
            "Test Accuracy: 90.58%\n",
            "Epoch [112/200], Loss: 0.0769, Train Accuracy: 97.29% Time: 6.604995489120483: \n",
            "Test Accuracy: 90.61%\n",
            "Epoch [113/200], Loss: 0.0809, Train Accuracy: 97.26% Time: 6.554358959197998: \n",
            "Test Accuracy: 90.49%\n",
            "Epoch [114/200], Loss: 0.0820, Train Accuracy: 97.17% Time: 6.366350173950195: \n",
            "Test Accuracy: 89.99%\n",
            "Epoch [115/200], Loss: 0.0774, Train Accuracy: 97.28% Time: 6.294723987579346: \n",
            "Test Accuracy: 90.10%\n",
            "Epoch [116/200], Loss: 0.0788, Train Accuracy: 97.30% Time: 6.602356910705566: \n",
            "Test Accuracy: 90.42%\n",
            "Epoch [117/200], Loss: 0.0792, Train Accuracy: 97.34% Time: 6.245867013931274: \n",
            "Test Accuracy: 90.13%\n",
            "Epoch [118/200], Loss: 0.0784, Train Accuracy: 97.26% Time: 6.4671642780303955: \n",
            "Test Accuracy: 90.49%\n",
            "Epoch [119/200], Loss: 0.0777, Train Accuracy: 97.35% Time: 6.312111854553223: \n",
            "Test Accuracy: 90.31%\n",
            "Epoch [120/200], Loss: 0.0827, Train Accuracy: 97.16% Time: 6.5232813358306885: \n",
            "Test Accuracy: 90.60%\n",
            "Epoch [121/200], Loss: 0.0761, Train Accuracy: 97.39% Time: 6.25566029548645: \n",
            "Test Accuracy: 90.31%\n",
            "Epoch [122/200], Loss: 0.0811, Train Accuracy: 97.15% Time: 6.193649768829346: \n",
            "Test Accuracy: 90.10%\n",
            "Epoch [123/200], Loss: 0.0736, Train Accuracy: 97.43% Time: 6.483546257019043: \n",
            "Test Accuracy: 90.47%\n",
            "Epoch [124/200], Loss: 0.0775, Train Accuracy: 97.35% Time: 6.237589359283447: \n",
            "Test Accuracy: 90.13%\n",
            "Epoch [125/200], Loss: 0.0784, Train Accuracy: 97.35% Time: 6.325330495834351: \n",
            "Test Accuracy: 89.98%\n",
            "Epoch [126/200], Loss: 0.0807, Train Accuracy: 97.20% Time: 6.540071964263916: \n",
            "Test Accuracy: 90.81%\n",
            "Epoch [127/200], Loss: 0.0771, Train Accuracy: 97.23% Time: 6.241343259811401: \n",
            "Test Accuracy: 90.62%\n",
            "Epoch [128/200], Loss: 0.0768, Train Accuracy: 97.33% Time: 6.483202934265137: \n",
            "Test Accuracy: 91.22%\n",
            "Epoch [129/200], Loss: 0.0731, Train Accuracy: 97.45% Time: 6.542113304138184: \n",
            "Test Accuracy: 90.31%\n",
            "Epoch [130/200], Loss: 0.0778, Train Accuracy: 97.33% Time: 6.308177709579468: \n",
            "Test Accuracy: 90.71%\n",
            "Epoch [131/200], Loss: 0.0761, Train Accuracy: 97.43% Time: 6.4273130893707275: \n",
            "Test Accuracy: 89.83%\n",
            "Epoch [132/200], Loss: 0.0777, Train Accuracy: 97.34% Time: 6.45668888092041: \n",
            "Test Accuracy: 90.40%\n",
            "Epoch [133/200], Loss: 0.0755, Train Accuracy: 97.39% Time: 6.5870256423950195: \n",
            "Test Accuracy: 91.16%\n",
            "Epoch [134/200], Loss: 0.0717, Train Accuracy: 97.56% Time: 6.511554002761841: \n",
            "Test Accuracy: 90.60%\n",
            "Epoch [135/200], Loss: 0.0769, Train Accuracy: 97.39% Time: 6.386530160903931: \n",
            "Test Accuracy: 90.43%\n",
            "Epoch [136/200], Loss: 0.0759, Train Accuracy: 97.39% Time: 6.850294351577759: \n",
            "Test Accuracy: 90.35%\n",
            "Epoch [137/200], Loss: 0.0764, Train Accuracy: 97.37% Time: 6.5983405113220215: \n",
            "Test Accuracy: 90.26%\n",
            "Epoch [138/200], Loss: 0.0741, Train Accuracy: 97.45% Time: 6.400365591049194: \n",
            "Test Accuracy: 89.34%\n",
            "Epoch [139/200], Loss: 0.0761, Train Accuracy: 97.35% Time: 6.758589029312134: \n",
            "Test Accuracy: 90.43%\n",
            "Epoch [140/200], Loss: 0.0741, Train Accuracy: 97.51% Time: 6.320561170578003: \n",
            "Test Accuracy: 90.24%\n",
            "Epoch [141/200], Loss: 0.0748, Train Accuracy: 97.38% Time: 6.499506711959839: \n",
            "Test Accuracy: 88.92%\n",
            "Epoch [142/200], Loss: 0.0734, Train Accuracy: 97.46% Time: 6.750448226928711: \n",
            "Test Accuracy: 90.63%\n",
            "Epoch [143/200], Loss: 0.0720, Train Accuracy: 97.46% Time: 6.738362550735474: \n",
            "Test Accuracy: 90.12%\n",
            "Epoch [144/200], Loss: 0.0751, Train Accuracy: 97.42% Time: 6.3555145263671875: \n",
            "Test Accuracy: 90.54%\n",
            "Epoch [145/200], Loss: 0.0763, Train Accuracy: 97.41% Time: 6.826117992401123: \n",
            "Test Accuracy: 90.79%\n",
            "Epoch [146/200], Loss: 0.0720, Train Accuracy: 97.51% Time: 6.394262790679932: \n",
            "Test Accuracy: 90.31%\n",
            "Epoch [147/200], Loss: 0.0734, Train Accuracy: 97.45% Time: 6.74273157119751: \n",
            "Test Accuracy: 90.67%\n",
            "Epoch [148/200], Loss: 0.0751, Train Accuracy: 97.27% Time: 6.538845062255859: \n",
            "Test Accuracy: 91.03%\n",
            "Epoch [149/200], Loss: 0.0702, Train Accuracy: 97.60% Time: 6.287886381149292: \n",
            "Test Accuracy: 90.35%\n",
            "Epoch [150/200], Loss: 0.0724, Train Accuracy: 97.47% Time: 6.979497671127319: \n",
            "Test Accuracy: 90.89%\n",
            "Epoch [151/200], Loss: 0.0722, Train Accuracy: 97.54% Time: 6.734496116638184: \n",
            "Test Accuracy: 90.47%\n",
            "Epoch [152/200], Loss: 0.0711, Train Accuracy: 97.57% Time: 6.35321044921875: \n",
            "Test Accuracy: 90.48%\n",
            "Epoch [153/200], Loss: 0.0754, Train Accuracy: 97.43% Time: 6.740230083465576: \n",
            "Test Accuracy: 90.57%\n",
            "Epoch [154/200], Loss: 0.0730, Train Accuracy: 97.51% Time: 6.56040620803833: \n",
            "Test Accuracy: 90.61%\n",
            "Epoch [155/200], Loss: 0.0731, Train Accuracy: 97.45% Time: 6.510574579238892: \n",
            "Test Accuracy: 91.02%\n",
            "Epoch [156/200], Loss: 0.0744, Train Accuracy: 97.42% Time: 6.851421117782593: \n",
            "Test Accuracy: 90.81%\n",
            "Epoch [157/200], Loss: 0.0723, Train Accuracy: 97.47% Time: 6.334005832672119: \n",
            "Test Accuracy: 90.75%\n",
            "Epoch [158/200], Loss: 0.0715, Train Accuracy: 97.53% Time: 7.12603759765625: \n",
            "Test Accuracy: 89.70%\n",
            "Epoch [159/200], Loss: 0.0701, Train Accuracy: 97.63% Time: 7.008699655532837: \n",
            "Test Accuracy: 91.00%\n",
            "Epoch [160/200], Loss: 0.0723, Train Accuracy: 97.53% Time: 6.493937253952026: \n",
            "Test Accuracy: 90.93%\n",
            "Epoch [161/200], Loss: 0.0748, Train Accuracy: 97.49% Time: 6.67435884475708: \n",
            "Test Accuracy: 90.75%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}